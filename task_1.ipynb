{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### **Task 1: Training a DQN Agent on the Racetrack Environment**\n",
    "- Implement a **Deep Q-Network (DQN) agent from scratch** using the provided configuration file (`config.py`).\n",
    "- Documentation on https://highway-env.farama.org/\n",
    "- The training process should be documented, including:\n",
    "  - Observations of different learning phases.\n",
    "  - Performance evaluation of the trained agent.\n",
    "  - Analysis of achieved rewards and behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import highway_env\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - DQN training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and configure environment\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.configure(config_dict)  # Use unwrapped environment to configure\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network model with flexible input dimensions.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize the DQN model.\n",
    "        \n",
    "        Args:\n",
    "            state_dim (tuple): Shape of the state space\n",
    "            action_dim (int): Number of possible actions\n",
    "            hidden_dim (int): Size of hidden layers\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Calculate flattened input dimension\n",
    "        self.state_dim = state_dim\n",
    "        flattened_dim = 1\n",
    "        for dim in state_dim:\n",
    "            flattened_dim *= dim\n",
    "        \n",
    "        # Network architecture\n",
    "        self.fc1 = nn.Linear(flattened_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Flatten the input if it's multi-dimensional\n",
    "        if len(x.shape) > 2:  # If we have batch dimension\n",
    "            x = x.view(x.size(0), -1)\n",
    "        else:\n",
    "            x = x.view(-1)\n",
    "            \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent implementation with experience replay and target network.\"\"\"\n",
    "    def __init__(self, env, config):\n",
    "        \"\"\"\n",
    "        Initialize the DQN Agent.\n",
    "        \n",
    "        Args:\n",
    "            env: The environment\n",
    "            config: Configuration dictionary with hyperparameters\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        \n",
    "        # Check and set state/action dimensions from environment\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.action_dim = env.action_space.n\n",
    "        print(f\"Initialized DQN Agent with state dim: {self.state_dim}, action dim: {self.action_dim}\")\n",
    "        \n",
    "        # Initialize Q-network and target network\n",
    "        self.q_network = DQN(self.state_dim, self.action_dim, config['hidden_dim'])\n",
    "        self.target_network = DQN(self.state_dim, self.action_dim, config['hidden_dim'])\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Initialize optimizer and replay buffer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config['lr'])\n",
    "        self.replay_buffer = deque(maxlen=config['buffer_size'])\n",
    "        \n",
    "        # Training variables\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = config['epsilon_start']\n",
    "        \n",
    "        # Device (GPU if available)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        \n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            eval_mode: If True, uses greedy policy (for evaluation)\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        if not eval_mode and random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Update epsilon using linear decay.\"\"\"\n",
    "        self.epsilon = max(\n",
    "            self.config['epsilon_end'],\n",
    "            self.config['epsilon_start'] - \n",
    "            (self.steps_done / self.config['epsilon_decay'])\n",
    "        )\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.append((\n",
    "            torch.FloatTensor(state),\n",
    "            torch.LongTensor([action]),\n",
    "            torch.FloatTensor([reward]),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor([done])\n",
    "        ))\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        \"\"\"Sample a batch of transitions from replay buffer.\"\"\"\n",
    "        batch = random.sample(self.replay_buffer, self.config['batch_size'])\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(states).to(self.device),\n",
    "            torch.stack(actions).to(self.device),\n",
    "            torch.stack(rewards).to(self.device),\n",
    "            torch.stack(next_states).to(self.device),\n",
    "            torch.stack(dones).to(self.device)\n",
    "        )\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step on a batch from replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) < self.config['batch_size']:\n",
    "            return 0  # Not enough samples yet\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "        \n",
    "        # Compute current Q values\n",
    "        current_q = self.q_network(states).gather(1, actions)\n",
    "        \n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1, keepdim=True)[0]\n",
    "            target_q = rewards + (1 - dones) * self.config['gamma'] * next_q\n",
    "        \n",
    "        # Compute loss and optimize\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.optimizer.zero_grad()  # Fixed: using correct attribute name\n",
    "        loss.backward()\n",
    "        self.optimizer.step()  # Fixed: using correct attribute name\n",
    "        \n",
    "        # Update target network\n",
    "        if self.steps_done % self.config['target_update'] == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.steps_done += 1\n",
    "        self.update_epsilon()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DQN Agent with state dim: (5, 5), action dim: 5\n",
      "Starting training...\n",
      "Episode   10 | Reward:   15.5 | Avg Reward (10):    9.9 | Length:  19 | Loss:   0.23 | Epsilon: 0.997\n",
      "Episode   20 | Reward:   10.3 | Avg Reward (10):    6.0 | Length:  13 | Loss:   0.06 | Epsilon: 0.993\n",
      "Episode   30 | Reward:    3.3 | Avg Reward (10):    9.0 | Length:   5 | Loss:   0.06 | Epsilon: 0.987\n",
      "Episode   40 | Reward:    3.9 | Avg Reward (10):    8.6 | Length:   6 | Loss:   0.06 | Epsilon: 0.981\n",
      "Episode   50 | Reward:   14.8 | Avg Reward (10):    7.0 | Length:  20 | Loss:   0.06 | Epsilon: 0.976\n",
      "Episode   60 | Reward:   23.3 | Avg Reward (10):    9.4 | Length:  30 | Loss:   0.06 | Epsilon: 0.970\n",
      "Episode   70 | Reward:   10.7 | Avg Reward (10):    7.7 | Length:  14 | Loss:   0.06 | Epsilon: 0.965\n",
      "Episode   80 | Reward:    4.1 | Avg Reward (10):    6.6 | Length:   6 | Loss:   0.06 | Epsilon: 0.961\n",
      "Episode   90 | Reward:    2.5 | Avg Reward (10):    4.8 | Length:   4 | Loss:   0.06 | Epsilon: 0.957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, config)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Plot training results\u001b[39;00m\n\u001b[0;32m    127\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "Cell \u001b[1;32mIn[21], line 42\u001b[0m, in \u001b[0;36mtrain_dqn_agent\u001b[1;34m(env, agent, config, n_episodes, save_dir)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Select and perform action\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m---> 42\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     done \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Store transition and train\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:242\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulate(action)\n\u001b[1;32m--> 242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n\u001b[0;32m    244\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_terminated()\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\highway_env\\envs\\common\\observation.py:262\u001b[0m, in \u001b[0;36mKinematicObservation.observe\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# Normalize and clip\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m--> 262\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Fill missing rows\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles_count:\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\highway_env\\envs\\common\\observation.py:228\u001b[0m, in \u001b[0;36mKinematicObservation.normalize_obs\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature, f_range \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_range\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m df:\n\u001b[1;32m--> 228\u001b[0m         \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mlmap(df[feature], [f_range[\u001b[38;5;241m0\u001b[39m], f_range[\u001b[38;5;241m1\u001b[39m]], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip:\n\u001b[0;32m    230\u001b[0m             df[feature] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(df[feature], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\pandas\\core\\frame.py:4538\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4535\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   4536\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4538\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\pandas\\core\\frame.py:4490\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value, refs)\n\u001b[0;32m   4489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4490\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4492\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   4493\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   4494\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[0;32m   4495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\pandas\\core\\frame.py:4478\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[1;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[0;32m   4470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[0;32m   4471\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4472\u001b[0m     loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4476\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4477\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[1;32m-> 4478\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\tedma\\miniconda3\\envs\\dl-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1133\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Accessing public blknos ensures the public versions are initialized\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[loc]\n\u001b[1;32m-> 1133\u001b[0m blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblklocs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m unfit_mgr_locs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1136\u001b[0m unfit_val_locs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def train_dqn_agent(env, agent, config, n_episodes=1000, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Train the DQN agent with logging and model saving.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        agent: DQN agent\n",
    "        config: Configuration dictionary\n",
    "        n_episodes: Number of training episodes\n",
    "        save_dir: Directory to save models and logs\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Training statistics\n",
    "    stats = defaultdict(list)\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_losses = []\n",
    "    epsilon_values = []\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select and perform action\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            done = done or truncated\n",
    "            \n",
    "            # Store transition and train\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            loss = agent.train_step()\n",
    "            \n",
    "            if loss:  # Only count loss if training happened\n",
    "                total_loss += loss\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Update statistics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step)\n",
    "        if total_loss > 0:  # Only append loss if training happened\n",
    "            avg_loss = total_loss / step\n",
    "            episode_losses.append(avg_loss)\n",
    "        else:\n",
    "            episode_losses.append(0)\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        \n",
    "        # Print episode summary\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_loss = np.mean(episode_losses[-10:]) if episode_losses[-10:] else 0\n",
    "            print(f\"Episode {episode:4d} | Reward: {total_reward:6.1f} | \"\n",
    "                  f\"Avg Reward (10): {avg_reward:6.1f} | Length: {step:3d} | \"\n",
    "                  f\"Loss: {avg_loss:6.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % 100 == 0:\n",
    "            model_path = os.path.join(save_dir, f\"dqn_episode_{episode}.pth\")\n",
    "            agent.save_model(model_path)\n",
    "            print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(save_dir, \"dqn_final.pth\")\n",
    "    agent.save_model(final_model_path)\n",
    "    \n",
    "    # Save training statistics\n",
    "    training_stats = {\n",
    "        'episode': list(range(1, n_episodes + 1)),\n",
    "        'reward': episode_rewards,\n",
    "        'length': episode_lengths,\n",
    "        'loss': episode_losses,\n",
    "        'epsilon': epsilon_values\n",
    "    }\n",
    "    stats_df = pd.DataFrame(training_stats)\n",
    "    stats_path = os.path.join(save_dir, \"training_stats.csv\")\n",
    "    stats_df.to_csv(stats_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {duration:.2f} seconds\")\n",
    "    print(f\"Final 10 episodes average reward: {np.mean(episode_rewards[-10:]):.1f}\")\n",
    "    print(f\"Models and stats saved to {save_dir}\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Default hyperparameters (can be adjusted)\n",
    "config = {\n",
    "    'hidden_dim': 128,           # Size of hidden layers in Q-network\n",
    "    'lr': 3e-4,                  # Learning rate\n",
    "    'buffer_size': 100000,       # Replay buffer size\n",
    "    'batch_size': 64,            # Batch size for training\n",
    "    'gamma': 0.99,              # Discount factor\n",
    "    'epsilon_start': 1.0,       # Initial exploration rate\n",
    "    'epsilon_end': 0.01,        # Minimum exploration rate\n",
    "    'epsilon_decay': 20000,      # Steps over which to decay epsilon\n",
    "    'target_update': 1000       # Steps between target network updates\n",
    "}\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "agent = DQNAgent(env, config)\n",
    "\n",
    "# Start training\n",
    "stats = train_dqn_agent(env, agent, config, n_episodes=500)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(stats['episode'], stats['reward'])\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(stats['episode'], stats['length'])\n",
    "plt.title('Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(stats['episode'], stats['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Loss')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(stats['episode'], stats['epsilon'])\n",
    "plt.title('Epsilon Values')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Observation of different learning phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Performance evaluation of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Analysis of achieved rewards and behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
